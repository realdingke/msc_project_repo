{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1618922414502,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "pdcb2itQoeCC",
    "outputId": "a834ba8b-56d0-452d-e9d5-76f9baab90a7"
   },
   "outputs": [],
   "source": [
    "# to run on colab\n",
    "import os, sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "# nb_path = '/content/notebooks'\n",
    "# os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
    "# sys.path.insert(0, nb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6348,
     "status": "ok",
     "timestamp": 1618922419081,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "ZKi2VYhaSc1U",
    "outputId": "b2912dcf-28a1-4b64-f004-81498cb9c9a0"
   },
   "outputs": [],
   "source": [
    "#!pip install --target=$nb_path aif360[all]\n",
    "!pip install aif360[all]\n",
    "#also the cw will make use two aif360 datasets: adult and german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6783,
     "status": "ok",
     "timestamp": 1618922419527,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "wqVUA3ZSn_9r",
    "outputId": "5985dcd7-1d8e-4ebc-8773-c027f169a9f2"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(r'/usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8261,
     "status": "ok",
     "timestamp": 1618922421324,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "xaj4BByHoArh",
    "outputId": "d5acd7ac-c46d-4b51-c505-d74ef2e1cd3f"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(r'/usr/local/lib/python3.7/dist-packages/aif360/data/raw/german')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/german/german.data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/german/german.data-numeric\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/german/german.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7562,
     "status": "ok",
     "timestamp": 1618922421325,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "ILspyXVSoBI3",
    "outputId": "44b16098-c85d-497e-b336-1d0522618976"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(r'/content/drive/MyDrive/cw2')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10744,
     "status": "ok",
     "timestamp": 1618922425021,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "HujH8F0tn4iS"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from aif360.datasets import AdultDataset, GermanDataset\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german\n",
    "# from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "# from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "#         import RejectOptionClassification\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "# from aif360.sklearn.metrics import disparate_impact_ratio\n",
    "# from aif360.sklearn.metrics import make_scorer\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "\n",
    "import pdb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  #MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10390,
     "status": "ok",
     "timestamp": 1618922425027,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "nFdiKQTZvdoF"
   },
   "outputs": [],
   "source": [
    "# set the controlled hyperparameters for lr and svm estimators\n",
    "SOLVER = 'liblinear'\n",
    "MAX_ITER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9865,
     "status": "ok",
     "timestamp": 1618922425028,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "1f-CLaqunyZH"
   },
   "outputs": [],
   "source": [
    "def to_dataframes(aif360_ds):\n",
    "    X = pd.DataFrame(aif360_ds.features, columns=aif360_ds.feature_names)\n",
    "    y = pd.Series(aif360_ds.labels.ravel(), name=aif360_ds.label_names[0])\n",
    "    return X, y\n",
    "\n",
    "def compare_lsts(l1, l2):\n",
    "    result = all(map(lambda x, y: x == y, l1, l2))\n",
    "    return result and len(l1) == len(l2)\n",
    "\n",
    "def sanity_check_before_nsf(dataset):\n",
    "    \"\"\"checks the integrity of data after the to_dataframes step by comparing it against original data\n",
    "    \"\"\"\n",
    "    X_1 = to_dataframes(dataset)[0].to_numpy()\n",
    "    y_1 = to_dataframes(dataset)[1].to_numpy()\n",
    "    X_2 = dataset.features\n",
    "    y_2 = dataset.labels\n",
    "    assert len(X_1) == len(X_2) and len(y_1) == len(y_2)\n",
    "    for i in range(len(X_1)):\n",
    "        assert compare_lsts(X_1[i], X_2[i])\n",
    "    for j in range(len(y_1)):\n",
    "        assert y_1[j] == y_2[j]\n",
    "\n",
    "    return None\n",
    "\n",
    "def train_model(model, dataset, sensitive_attr=None):\n",
    "    scaler = StandardScaler()\n",
    "    if sensitive_attr is None:\n",
    "        X = scaler.fit_transform(dataset.features)\n",
    "    else:\n",
    "        #sanity_check_before_nsf(dataset)\n",
    "        # X_nsf is the input features with no sensitive features\n",
    "        X_nsf = to_dataframes(dataset)[0].drop(columns=sensitive_attr).to_numpy()\n",
    "        X = scaler.fit_transform(X_nsf)\n",
    "        \n",
    "    y = dataset.labels.ravel()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataset, unprivileged_groups, privileged_groups, sensitive_attr=None):\n",
    "    scaler = StandardScaler()\n",
    "    if sensitive_attr is None:\n",
    "        X = scaler.fit_transform(dataset.features)\n",
    "    else:\n",
    "        # X_nsf is the input features with no sensitive features\n",
    "        X_nsf = to_dataframes(dataset)[0].drop(columns=sensitive_attr).to_numpy()\n",
    "        X = scaler.fit_transform(X_nsf)\n",
    "    \n",
    "    y = dataset.labels.ravel()\n",
    "    predictions = model.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    dataset_pred = dataset.copy()\n",
    "    dataset_pred.labels = predictions\n",
    "    fairness_metrics = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, \n",
    "                                            privileged_groups=privileged_groups)\n",
    "#     disparate_impact = fairness_metrics.disparate_impact()\n",
    "#     stats_parity_diff = fairness_metrics.statistical_parity_difference()\n",
    "    eq_opp_diff = fairness_metrics.equal_opportunity_difference()\n",
    "    avr_odds_diff = fairness_metrics.average_odds_difference()\n",
    "    return {'accuracy': accuracy, 'eq_opp_diff': eq_opp_diff, 'avr_odds_diff': avr_odds_diff}\n",
    "\n",
    "def split_aif360_dataset(dataset, num_fold):\n",
    "    cumulative_ratio = 0\n",
    "    ratio_lst = []\n",
    "    for i in range(num_fold-1):\n",
    "        cumulative_ratio += 1/num_fold\n",
    "        ratio_lst.append(cumulative_ratio)\n",
    "    return dataset.split(ratio_lst, shuffle=True)\n",
    "\n",
    "def get_s_ratio_lst(num_fold):\n",
    "    \"\"\"get the split ratios given total number of folds\"\"\"\n",
    "    cumulative_ratio = 0\n",
    "    ratio_lst = []\n",
    "    for i in range(num_fold-1):\n",
    "        cumulative_ratio += 1/num_fold\n",
    "        ratio_lst.append(cumulative_ratio)\n",
    "        \n",
    "    return ratio_lst\n",
    "\n",
    "def get_fold_idx_iterables(dataset, num_fold):\n",
    "    \"\"\"get the fold index list iterables as a nested list(each corresponds to each fold)\n",
    "       args: dataset:aif360 dataset; num_fold: total number of folds > 2 (if = 2 use split_aif360_dataset)\n",
    "    \"\"\"\n",
    "    ratio_lst = get_s_ratio_lst(num_fold)\n",
    "    idx_iters = []\n",
    "    for n, perc in enumerate(ratio_lst):\n",
    "        if n == 0:\n",
    "            idx_iters.append([idx for idx in range(int(len(dataset.features)*perc))])\n",
    "            continue\n",
    "        \n",
    "        idx_iters.append(\n",
    "            [idx for idx in range(int(len(dataset.features)*ratio_lst[n-1]), int(len(dataset.features)*perc))]\n",
    "            )\n",
    "        \n",
    "        if n == len(ratio_lst) - 1:\n",
    "            idx_iters.append(\n",
    "                [idx for idx in range(int(len(dataset.features)*perc), int(len(dataset.features)))]\n",
    "                )\n",
    "            break\n",
    "    \n",
    "    return idx_iters\n",
    "\n",
    "\n",
    "def combined_metric(acc, fairness):\n",
    "    \"\"\"for task 3 model selection criterion: define the new criterion by assigning weights to accuracy and fairness\"\"\"\n",
    "    acc_weight = 1\n",
    "    fairness_weight = -1.5\n",
    "    return acc_weight*acc + fairness_weight*fairness\n",
    "\n",
    "def cv_results_vis(results, C_val_array, tasknum):\n",
    "    \"\"\"Visualizes the model selection process\"\"\"\n",
    "    lr_acc_results = [acc for _,acc,_ in results[:len(results)//2]]\n",
    "    lr_eq_opp_diff_results = [eq_opp_diff for _,_,eq_opp_diff in results[:len(results)//2]]\n",
    "    svm_acc_results = [acc for _,acc,_ in results[len(results)//2:]]\n",
    "    svm_eq_opp_diff_results = [eq_opp_diff for _,_,eq_opp_diff in results[len(results)//2:]]\n",
    "    lr_combined_results = [combined_metric(acc, abs(fair)) for acc, fair in zip(lr_acc_results, lr_eq_opp_diff_results)]\n",
    "    svm_combined_results = [combined_metric(acc, abs(fair)) for acc, fair in zip(svm_acc_results, svm_eq_opp_diff_results)]\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax1 = fig.add_subplot(211)\n",
    "#     ax2 = fig.add_subplot(212)\n",
    "\n",
    "    fig, axes = plt.subplots(2,1, figsize=(14,14))\n",
    "    \n",
    "    axes[0].scatter(C_val_array, lr_acc_results, c='yellow')\n",
    "    axes[0].scatter(C_val_array, svm_acc_results, c='red')\n",
    "    axes[0].plot(C_val_array, lr_acc_results, color='yellow', label='lr')\n",
    "    axes[0].plot(C_val_array, svm_acc_results, color='red', label='svm')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid()\n",
    "    axes[0].semilogx(basex=10)\n",
    "    axes[0].set_xlabel(\"hyperparameter C in logscale with a logbase of 10\", fontsize=15)\n",
    "    axes[0].set_ylabel(\"accuracy\", fontsize=15)\n",
    "    axes[0].set_title(\"Plot of hyperparameter against accuracy metric\", weight='bold')\n",
    "    \n",
    "    axes[1].scatter(C_val_array, lr_eq_opp_diff_results, c='yellow')\n",
    "    axes[1].scatter(C_val_array, svm_eq_opp_diff_results, c='red')\n",
    "    axes[1].plot(C_val_array, lr_eq_opp_diff_results, color='yellow', label='lr')\n",
    "    axes[1].plot(C_val_array, svm_eq_opp_diff_results, color='red', label='svm')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "    axes[1].semilogx(basex=10)\n",
    "    axes[1].set_xlabel(\"hyperparameter C in logscale with a logbase of 10\", fontsize=15)\n",
    "    axes[1].set_ylabel(\"equality of opportunity difference\", fontsize=15)\n",
    "    axes[1].set_title(\"Plot of hyperparameter against fairness metric\", weight='bold')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "#     plt.subplot_tool()\n",
    "    plt.savefig(f\"task_{tasknum}_model_selection.png\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # also need to plot for the task 3 model selection at this stage\n",
    "    task3_fig = plt.figure(figsize =(13, 13))\n",
    "    plt.scatter(C_val_array, lr_combined_results, c='yellow')\n",
    "    plt.scatter(C_val_array, svm_combined_results, c='red')\n",
    "    plt.plot(C_val_array, lr_combined_results, color='yellow', label='lr')\n",
    "    plt.plot(C_val_array, svm_combined_results, color='red', label='svm')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.semilogx(basex=10)\n",
    "    plt.xlabel(\"hyperparameter C in logscale with a logbase of 10\", fontsize=14)\n",
    "    plt.ylabel(\"combined accuracy and fairness metric\", fontsize=14)\n",
    "    plt.title(\"Plot of hyperparameter against combined metric\", weight='bold')\n",
    "    \n",
    "    plt.savefig(f\"task_3_model_selection_with_task_{tasknum}_data.png\")\n",
    "    plt.show()\n",
    "    \n",
    "            \n",
    "    return None\n",
    "\n",
    "def cross_validate_search(dataset, \n",
    "                   estimators, \n",
    "                   params, \n",
    "                   fold_num, \n",
    "                   unprivileged_groups, \n",
    "                   privileged_groups,\n",
    "                   sensitive_attr=None,\n",
    "                   visualization=True):\n",
    "    \"\"\"\n",
    "    Perform multi-model cross validation and hyperparam search with fold_num number of folds\n",
    "    \"\"\"\n",
    "    idx_iters = get_fold_idx_iterables(dataset, fold_num)\n",
    "    \n",
    "    # initialize metric data arrays\n",
    "    accuracy_array = np.zeros(len(params['lr']['lr_C']) + len(params['svm']['svm_C'])).reshape(1, -1)\n",
    "    eq_opp_diff_array = np.zeros(len(params['lr']['lr_C']) + len(params['svm']['svm_C'])).reshape(1, -1)\n",
    "    \n",
    "    for test_id in range(fold_num):\n",
    "        test_idx_lst = idx_iters[test_id]\n",
    "        train_idx_lst = []\n",
    "        for idx, idx_iter in enumerate(idx_iters): \n",
    "            if idx!=test_id:\n",
    "                train_idx_lst += idx_iter\n",
    "        \n",
    "        # now can get the test and train dataset under this fold\n",
    "        test_dataset = dataset.subset(test_idx_lst)\n",
    "        train_dataset =  dataset.subset(train_idx_lst)\n",
    "        \n",
    "        # print(len(train_dataset.features), len(test_dataset.features))\n",
    "        \n",
    "        # initialize the list of models each with its hyperparam choice\n",
    "        lr_model_lst = [estimators['lr'](C=C_val, solver=SOLVER, random_state=1) for C_val in params['lr']['lr_C']]\n",
    "        svm_model_lst = [estimators['svm'](C=C_val, max_iter=MAX_ITER, random_state=1) for C_val in params['svm']['svm_C']]\n",
    "        model_lst = lr_model_lst + svm_model_lst\n",
    "        \n",
    "        # train the model list\n",
    "        if sensitive_attr is None:\n",
    "            trained_model_lst = [train_model(model, train_dataset) for model in model_lst]\n",
    "        else:\n",
    "            trained_model_lst = [train_model(model, train_dataset, sensitive_attr=sensitive_attr) for model in model_lst]\n",
    "        \n",
    "        # test the trained models on test dataset\n",
    "        if sensitive_attr is None:\n",
    "            accuracy_data = np.array([evaluate(model, test_dataset, unprivileged_groups, privileged_groups)['accuracy'] \n",
    "                             for model in trained_model_lst]).reshape(1,-1)\n",
    "            eq_opp_diff_data = np.array([evaluate(model, test_dataset, unprivileged_groups, privileged_groups)['eq_opp_diff'] \n",
    "                                for model in trained_model_lst]).reshape(1,-1)\n",
    "        else:\n",
    "            accuracy_data = np.array([evaluate(model, test_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)['accuracy'] \n",
    "                             for model in trained_model_lst]).reshape(1,-1)\n",
    "            eq_opp_diff_data = np.array([evaluate(model, test_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)['eq_opp_diff'] \n",
    "                                for model in trained_model_lst]).reshape(1,-1)\n",
    "\n",
    "        accuracy_array = np.concatenate((accuracy_array, accuracy_data), axis=0)\n",
    "        eq_opp_diff_array = np.concatenate((eq_opp_diff_array, eq_opp_diff_data), axis=0)\n",
    "    \n",
    "    \n",
    "    # get the average results from all folds\n",
    "    accuracy_array = np.sum(accuracy_array, axis=0)/fold_num\n",
    "    eq_opp_diff_array = np.sum(eq_opp_diff_array, axis=0)/fold_num\n",
    "    \n",
    "    results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "    \n",
    "    # generate the result table model information column (first column)\n",
    "    info_lst = [key + f\"_{np.round(C, 4)}\" for dct in params.values() for key, val in dct.items() for C in val]\n",
    "        \n",
    "    results += [\n",
    "               [model, accuracy, eq_opp_diff]\n",
    "               for model, accuracy, eq_opp_diff in zip(info_lst, accuracy_array, eq_opp_diff_array)\n",
    "            ]\n",
    "    \n",
    "    # plot the C_vals against the acc and fairness results, for all models\n",
    "    if visualization:\n",
    "        cv_results_vis(results[1:], params['lr']['lr_C'], 1)\n",
    "    \n",
    "    return pd.DataFrame(results[1:], columns=results[0]).set_index('model').sort_values(by='accuracy', ascending=False), \\\n",
    "           pd.DataFrame(results[1:], columns=results[0]).set_index('model').sort_values(by='eq_opp_diff', key=lambda x: abs(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9320,
     "status": "ok",
     "timestamp": 1618922425451,
     "user": {
      "displayName": "KE DING",
      "photoUrl": "",
      "userId": "07359568246475612991"
     },
     "user_tz": -480
    },
    "id": "VAQ7wGwjnyZb"
   },
   "outputs": [],
   "source": [
    "#with reweighing\n",
    "def train_model_w_reweigh(model, dataset, unprivileged_groups, privileged_groups, sensitive_attr=None):\n",
    "    scaler = StandardScaler()\n",
    "    if sensitive_attr is None:\n",
    "        X = scaler.fit_transform(dataset.features)\n",
    "    else:\n",
    "        # X_nsf is the input features with no sensitive features\n",
    "        X_nsf = to_dataframes(dataset)[0].drop(columns=sensitive_attr).to_numpy()\n",
    "        X = scaler.fit_transform(X_nsf)\n",
    "    \n",
    "    y = dataset.labels.ravel()\n",
    "    \n",
    "    # reweighing\n",
    "    RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "    #We obtain a set of weights for the training set, to use in scikit-learn.\n",
    "    dataset = RW.fit_transform(dataset)\n",
    "    \n",
    "    model.fit(X, y, sample_weight=dataset.instance_weights)\n",
    "    return model\n",
    "\n",
    "def evaluate_w_reweigh(model, dataset, unprivileged_groups, privileged_groups, sensitive_attr=None):\n",
    "    \"\"\"the evaluation for reweighing fairness method, no actual reweighing of test data, \n",
    "       but need to add an additional resize step to predictions\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    if sensitive_attr is None:\n",
    "        X = scaler.fit_transform(dataset.features)\n",
    "    else:\n",
    "        # X_nsf is the input features with no sensitive features\n",
    "        X_nsf = to_dataframes(dataset)[0].drop(columns=sensitive_attr).to_numpy()\n",
    "        X = scaler.fit_transform(X_nsf)\n",
    "    \n",
    "    y = dataset.labels.ravel()\n",
    "    predictions = model.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    dataset_pred = dataset.copy()\n",
    "    predictions.resize((len(predictions),1))\n",
    "    dataset_pred.labels = predictions\n",
    "    fairness_metrics = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, \n",
    "                                            privileged_groups=privileged_groups)\n",
    "#     disparate_impact = fairness_metrics.disparate_impact()\n",
    "#     stats_parity_diff = fairness_metrics.statistical_parity_difference()\n",
    "    eq_opp_diff = fairness_metrics.equal_opportunity_difference()\n",
    "    avr_odds_diff = fairness_metrics.average_odds_difference()\n",
    "    return {'accuracy': accuracy, 'eq_opp_diff': eq_opp_diff, 'avr_odds_diff': avr_odds_diff}\n",
    "\n",
    "def cross_validate_search_w_reweigh(dataset, \n",
    "                   estimators, \n",
    "                   params, \n",
    "                   fold_num, \n",
    "                   unprivileged_groups, \n",
    "                   privileged_groups,\n",
    "                   sensitive_attr=None,\n",
    "                   visualization=True):\n",
    "    \"\"\"\n",
    "    Perform multi-model cross validation and hyperparam search with fold_num number of folds\n",
    "    \"\"\"\n",
    "    idx_iters = get_fold_idx_iterables(dataset, fold_num)\n",
    "    \n",
    "    # initialize metric data arrays\n",
    "    accuracy_array = np.zeros(len(params['lr']['lr_C']) + len(params['svm']['svm_C'])).reshape(1, -1)\n",
    "    eq_opp_diff_array = np.zeros(len(params['lr']['lr_C']) + len(params['svm']['svm_C'])).reshape(1, -1)\n",
    "    \n",
    "    for test_id in range(fold_num):\n",
    "        test_idx_lst = idx_iters[test_id]\n",
    "        train_idx_lst = []\n",
    "        for idx, idx_iter in enumerate(idx_iters): \n",
    "            if idx!=test_id:\n",
    "                train_idx_lst += idx_iter\n",
    "        \n",
    "        # now can get the test and train dataset under this fold\n",
    "        test_dataset = dataset.subset(test_idx_lst)\n",
    "        train_dataset =  dataset.subset(train_idx_lst)\n",
    "        \n",
    "        \n",
    "        # initialize the list of models each with its hyperparam choice\n",
    "        lr_model_lst = [estimators['lr'](C=C_val, solver=SOLVER, random_state=1) for C_val in params['lr']['lr_C']]\n",
    "        svm_model_lst = [estimators['svm'](C=C_val, max_iter=MAX_ITER, random_state=1) for C_val in params['svm']['svm_C']]\n",
    "        model_lst = lr_model_lst + svm_model_lst\n",
    "        \n",
    "        # train the model list\n",
    "        if sensitive_attr is None:\n",
    "            trained_model_lst = [train_model_w_reweigh(model, train_dataset, unprivileged_groups, privileged_groups) \n",
    "                                 for model in model_lst]\n",
    "        else:\n",
    "            trained_model_lst = [train_model_w_reweigh(model, train_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr) \n",
    "                                 for model in model_lst]\n",
    "        \n",
    "        # test the trained models on test dataset\n",
    "        if sensitive_attr is None:\n",
    "            accuracy_data = np.array([evaluate_w_reweigh(model, test_dataset, unprivileged_groups, privileged_groups)['accuracy'] \n",
    "                             for model in trained_model_lst]).reshape(1,-1)\n",
    "            eq_opp_diff_data = np.array([evaluate_w_reweigh(model, test_dataset, unprivileged_groups, privileged_groups)['eq_opp_diff'] \n",
    "                                for model in trained_model_lst]).reshape(1,-1)\n",
    "        else:\n",
    "            accuracy_data = np.array([evaluate_w_reweigh(model, test_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)['accuracy'] \n",
    "                             for model in trained_model_lst]).reshape(1,-1)\n",
    "            eq_opp_diff_data = np.array([evaluate_w_reweigh(model, test_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)['eq_opp_diff'] \n",
    "                                for model in trained_model_lst]).reshape(1,-1)\n",
    "        \n",
    "        accuracy_array = np.concatenate((accuracy_array, accuracy_data), axis=0)\n",
    "        eq_opp_diff_array = np.concatenate((eq_opp_diff_array, eq_opp_diff_data), axis=0)\n",
    "    \n",
    "    \n",
    "    # get the average results from all folds\n",
    "    accuracy_array = np.sum(accuracy_array, axis=0)/fold_num\n",
    "    eq_opp_diff_array = np.sum(eq_opp_diff_array, axis=0)/fold_num\n",
    "    \n",
    "    results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "    \n",
    "    # generate the result table model information column (first column)\n",
    "    info_lst = [key + f\"_{np.round(C, 4)}\" for dct in params.values() for key, val in dct.items() for C in val]\n",
    "        \n",
    "    results += [\n",
    "               [model, accuracy, eq_opp_diff]\n",
    "               for model, accuracy, eq_opp_diff in zip(info_lst, accuracy_array, eq_opp_diff_array)\n",
    "            ]\n",
    "    \n",
    "    # plot the C_vals against the acc and fairness results, for all models\n",
    "    if visualization:\n",
    "        cv_results_vis(results[1:], params['lr']['lr_C'], 2)\n",
    "    \n",
    "    return pd.DataFrame(results[1:], columns=results[0]).set_index('model').sort_values(by='accuracy', ascending=False), \\\n",
    "           pd.DataFrame(results[1:], columns=results[0]).set_index('model').sort_values(by='eq_opp_diff', key=lambda x: abs(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with Equalized Odds Post-processing\n",
    "def evaluate_w_eop(model, dataset, unprivileged_groups, privileged_groups, sensitive_attr=None):\n",
    "    scaler = StandardScaler()\n",
    "    if sensitive_attr is None:\n",
    "        X = scaler.fit_transform(dataset.features)\n",
    "    else:\n",
    "        # X_nsf is the input features with no sensitive features\n",
    "        X_nsf = to_dataframes(dataset)[0].drop(columns=sensitive_attr).to_numpy()\n",
    "        X = scaler.fit_transform(X_nsf)\n",
    "    \n",
    "    y = dataset.labels.ravel()\n",
    "    predictions = model.predict(X)\n",
    "    dataset_pred = dataset.copy()\n",
    "    predictions.resize((len(predictions),1))\n",
    "    dataset_pred.labels = predictions\n",
    "    # apply the Equalized Odds post-processing method\n",
    "    eop = EqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     seed=0)\n",
    "    eop = eop.fit(dataset, dataset_pred)\n",
    "    transformed_dataset_pred = eop.predict(dataset_pred)\n",
    "    accuracy = accuracy_score(y,  transformed_dataset_pred.labels.ravel())\n",
    "    fairness_metrics = ClassificationMetric(dataset, transformed_dataset_pred, unprivileged_groups=unprivileged_groups, \n",
    "                                            privileged_groups=privileged_groups)\n",
    "#     disparate_impact = fairness_metrics.disparate_impact()\n",
    "#     stats_parity_diff = fairness_metrics.statistical_parity_difference()\n",
    "    eq_opp_diff = fairness_metrics.equal_opportunity_difference()\n",
    "    avr_odds_diff = fairness_metrics.average_odds_difference()\n",
    "    return {'accuracy': accuracy, 'eq_opp_diff': eq_opp_diff, 'avr_odds_diff': avr_odds_diff}\n",
    "\n",
    "def cross_validate_search_w_eop(dataset, \n",
    "                   estimators, \n",
    "                   params, \n",
    "                   fold_num, \n",
    "                   unprivileged_groups, \n",
    "                   privileged_groups,\n",
    "                   sensitive_attr=None,\n",
    "                   visualization=True):\n",
    "    \"\"\"\n",
    "    Perform multi-model cross validation and hyperparam search with fold_num number of folds\n",
    "    \"\"\"\n",
    "    idx_iters = get_fold_idx_iterables(dataset, fold_num)\n",
    "    \n",
    "    # initialize metric data arrays\n",
    "    accuracy_array = np.zeros(len(params['lr']['lr_C']) + len(params['svm']['svm_C'])).reshape(1, -1)\n",
    "    eq_opp_diff_array = np.zeros(len(params['lr']['lr_C']) + len(params['svm']['svm_C'])).reshape(1, -1)\n",
    "    \n",
    "    for test_id in range(fold_num):\n",
    "        test_idx_lst = idx_iters[test_id]\n",
    "        train_idx_lst = []\n",
    "        for idx, idx_iter in enumerate(idx_iters): \n",
    "            if idx!=test_id:\n",
    "                train_idx_lst += idx_iter\n",
    "        \n",
    "        # now can get the test and train dataset under this fold\n",
    "        test_dataset = dataset.subset(test_idx_lst)\n",
    "        train_dataset =  dataset.subset(train_idx_lst)\n",
    "        \n",
    "        # print(len(train_dataset.features), len(test_dataset.features))\n",
    "        \n",
    "        # initialize the list of models each with its hyperparam choice\n",
    "        lr_model_lst = [estimators['lr'](C=C_val, solver=SOLVER, random_state=1) for C_val in params['lr']['lr_C']]\n",
    "        svm_model_lst = [estimators['svm'](C=C_val, max_iter=MAX_ITER, random_state=1) for C_val in params['svm']['svm_C']]\n",
    "        model_lst = lr_model_lst + svm_model_lst\n",
    "        \n",
    "        # train the model list\n",
    "        if sensitive_attr is None:\n",
    "            trained_model_lst = [train_model(model, train_dataset) for model in model_lst]\n",
    "        else:\n",
    "            trained_model_lst = [train_model(model, train_dataset, sensitive_attr=sensitive_attr) for model in model_lst]\n",
    "        \n",
    "        # test the trained models on test dataset\n",
    "        if sensitive_attr is None:\n",
    "            accuracy_data = np.array([evaluate_w_eop(model, test_dataset, unprivileged_groups, privileged_groups)['accuracy'] \n",
    "                             for model in trained_model_lst]).reshape(1,-1)\n",
    "            eq_opp_diff_data = np.array([evaluate_w_eop(model, test_dataset, unprivileged_groups, privileged_groups)['eq_opp_diff'] \n",
    "                                for model in trained_model_lst]).reshape(1,-1)\n",
    "        else:\n",
    "            accuracy_data = np.array([evaluate_w_eop(model, test_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)['accuracy'] \n",
    "                             for model in trained_model_lst]).reshape(1,-1)\n",
    "            eq_opp_diff_data = np.array([evaluate_w_eop(model, test_dataset, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)['eq_opp_diff'] \n",
    "                                for model in trained_model_lst]).reshape(1,-1)\n",
    "\n",
    "        accuracy_array = np.concatenate((accuracy_array, accuracy_data), axis=0)\n",
    "        eq_opp_diff_array = np.concatenate((eq_opp_diff_array, eq_opp_diff_data), axis=0)\n",
    "    \n",
    "    \n",
    "    # get the average results from all folds\n",
    "    accuracy_array = np.sum(accuracy_array, axis=0)/fold_num\n",
    "    eq_opp_diff_array = np.sum(eq_opp_diff_array, axis=0)/fold_num\n",
    "    \n",
    "    results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "    \n",
    "    # generate the result table model information column (first column)\n",
    "    info_lst = [key + f\"_{np.round(C, 4)}\" for dct in params.values() for key, val in dct.items() for C in val]\n",
    "        \n",
    "    results += [\n",
    "               [model, accuracy, eq_opp_diff]\n",
    "               for model, accuracy, eq_opp_diff in zip(info_lst, accuracy_array, eq_opp_diff_array)\n",
    "            ]\n",
    "    \n",
    "    # plot the C_vals against the acc and fairness results, for all models\n",
    "    if visualization:\n",
    "        cv_results_vis(results[1:], params['lr']['lr_C'], 1)\n",
    "    \n",
    "    return pd.DataFrame(results[1:], columns=results[0]).set_index('model').sort_values(by='accuracy', ascending=False), \\\n",
    "           pd.DataFrame(results[1:], columns=results[0]).set_index('model').sort_values(by='eq_opp_diff', key=lambda x: abs(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zd_pxwVuCbGS",
    "outputId": "94e37cca-97ad-42f0-b712-848139d0aa30"
   },
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def full_run_for_ds(rseeds,\n",
    "                    privileged_groups,\n",
    "                    unprivileged_groups,\n",
    "                    dataset_orig,\n",
    "                    estimators, \n",
    "                    parameters, \n",
    "                    cv_fold_num, \n",
    "                    save_results=False):\n",
    "    \"\"\"for a specified dataset, runs the entirety of the 3 tasks' pipelines and repeat for 5 times,\n",
    "       each time with an initial split controlled by the random seed from the rseeds list\n",
    "    \"\"\"\n",
    "    \n",
    "    final_results = [['repeats',\n",
    "                      'model_1_accuracy',\n",
    "                      'model_1_eq_opp_diff',\n",
    "                      'model_2_accuracy',\n",
    "                      'model_2_eq_opp_diff',\n",
    "                      'model_3_accuracy',\n",
    "                      'model_3_eq_opp_diff',\n",
    "                      'model_4_accuracy',\n",
    "                      'model_4_eq_opp_diff',\n",
    "                      'model_5_accuracy',\n",
    "                      'model_5_eq_opp_diff',\n",
    "                      'model_6_accuracy',\n",
    "                      'model_6_eq_opp_diff']]\n",
    "\n",
    "    for i, rseed in enumerate(rseeds):\n",
    "        repeat_num = i + 1\n",
    "\n",
    "        results_per_repeat = [f\"Repeat_{repeat_num}\"]\n",
    "\n",
    "        # set the random seed\n",
    "        np.random.seed(rseed)\n",
    "\n",
    "        # task 1: first shuffle and split the whole dataset into train-test\n",
    "        train, test = dataset_orig.split([0.7], shuffle=True)\n",
    "        print('\\nTotal number of training examples:', len(train.features))\n",
    "\n",
    "\n",
    "        \n",
    "        task_1_results_dataframes = cross_validate_search(train, \n",
    "                      estimators, \n",
    "                      parameters, \n",
    "                      cv_fold_num, \n",
    "                      unprivileged_groups, \n",
    "                      privileged_groups,\n",
    "                      visualization=True)\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the task 1 cv results are:\\n\\n\", task_1_results_dataframes[0], '\\n\\n', task_1_results_dataframes[1])\n",
    "        \n",
    "        \n",
    "        # test the selected models on held-out test dataset\n",
    "        # best model based on accuracy(model 1):\n",
    "        acc_model = task_1_results_dataframes[0].reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_1 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_1 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # best model based on fairness(model 2):\n",
    "        fair_model = task_1_results_dataframes[1].reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_2 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_2 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # train the selected models\n",
    "        model_1_trained = train_model(model_1, train)\n",
    "        model_2_trained = train_model(model_2, train)\n",
    "\n",
    "        model_1_results = evaluate(model_1_trained, test, unprivileged_groups, privileged_groups)\n",
    "        model_2_results = evaluate(model_2_trained, test, unprivileged_groups, privileged_groups)\n",
    "\n",
    "        results_per_repeat.append(model_1_results['accuracy'])\n",
    "        results_per_repeat.append(model_1_results['eq_opp_diff'])\n",
    "        results_per_repeat.append(model_2_results['accuracy'])\n",
    "        results_per_repeat.append(model_2_results['eq_opp_diff'])\n",
    "\n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_1: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_1_results['accuracy'],\n",
    "                        model_1_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_2: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_2_results['accuracy'],\n",
    "                        model_2_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_1 & model_2 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_1_results.csv\")\n",
    "\n",
    "\n",
    "        # task 2 with reweighing\n",
    "        task_2_results_dataframes = cross_validate_search_w_reweigh(train, \n",
    "               estimators, \n",
    "               parameters, \n",
    "               cv_fold_num, \n",
    "               unprivileged_groups, \n",
    "               privileged_groups,\n",
    "               visualization=True)\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the task 2 cv results are:\\n\\n\", task_2_results_dataframes[0], '\\n\\n', task_2_results_dataframes[1])\n",
    "        \n",
    "        # test the selected models from task 2 on held-out test dataset\n",
    "        # best model based on accuracy(model 3):\n",
    "        acc_model = task_2_results_dataframes[0].reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_3 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_3 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # best model based on fairness(model 4):\n",
    "        fair_model = task_2_results_dataframes[1].reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_4 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_4 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # train the selected models\n",
    "        model_3_trained = train_model_w_reweigh(model_3, train, unprivileged_groups, privileged_groups)\n",
    "        model_4_trained = train_model_w_reweigh(model_4, train, unprivileged_groups, privileged_groups)\n",
    "\n",
    "        model_3_results = evaluate_w_reweigh(model_3_trained, test, unprivileged_groups, privileged_groups)\n",
    "        model_4_results = evaluate_w_reweigh(model_4_trained, test, unprivileged_groups, privileged_groups)\n",
    "\n",
    "        results_per_repeat.append(model_3_results['accuracy'])\n",
    "        results_per_repeat.append(model_3_results['eq_opp_diff'])\n",
    "        results_per_repeat.append(model_4_results['accuracy'])\n",
    "        results_per_repeat.append(model_4_results['eq_opp_diff'])\n",
    "\n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_3: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_3_results['accuracy'],\n",
    "                        model_3_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_4: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_4_results['accuracy'],\n",
    "                        model_4_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_3 & model_4 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_2_results.csv\")\n",
    "        \n",
    "        \n",
    "        # Task 3: model selection based on both accuracy and fairness\n",
    "        \n",
    "        # first for the fairness method(reweighing from task2) based model(model 5)\n",
    "        combined_fairness_df = task_2_results_dataframes[0]\n",
    "        combined_fairness_df['combinedmetric'] = combined_metric(combined_fairness_df['accuracy'], \n",
    "                                                                 abs(combined_fairness_df['eq_opp_diff']))\n",
    "        combined_fairness_df = combined_fairness_df.sort_values(by='combinedmetric', ascending=False)\n",
    "        \n",
    "        # select the model 5 as the best model based on combinedmetric result using task 2 method\n",
    "        fair_model = combined_fairness_df.reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_5 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_5 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "        \n",
    "        # train and test the model 5\n",
    "        model_5_trained = train_model_w_reweigh(model_5, train, unprivileged_groups, privileged_groups)\n",
    "        model_5_results = evaluate_w_reweigh(model_5_trained, test, unprivileged_groups, privileged_groups)\n",
    "        \n",
    "        results_per_repeat.append(model_5_results['accuracy'])\n",
    "        results_per_repeat.append(model_5_results['eq_opp_diff'])\n",
    "        \n",
    "        # now for the standard method model(from task 1)\n",
    "        combined_standard_df = task_1_results_dataframes[0]\n",
    "        combined_standard_df['combinedmetric'] = combined_metric(combined_standard_df['accuracy'], \n",
    "                                                                 abs(combined_standard_df['eq_opp_diff']))\n",
    "        combined_standard_df = combined_standard_df.sort_values(by='combinedmetric', ascending=False)\n",
    "        \n",
    "        # select the model 6 as the best model based on combinedmetric result using task 1 method\n",
    "        acc_model = combined_standard_df.reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_6 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_6 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "        \n",
    "        # train and test the model 6\n",
    "        model_6_trained = train_model(model_6, train)\n",
    "        model_6_results = evaluate(model_6_trained, test, unprivileged_groups, privileged_groups)\n",
    "        \n",
    "        results_per_repeat.append(model_6_results['accuracy'])\n",
    "        results_per_repeat.append(model_6_results['eq_opp_diff'])\n",
    "        \n",
    "        print(combined_fairness_df)\n",
    "        print()\n",
    "        print(combined_standard_df)\n",
    "        \n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_5: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_5_results['accuracy'],\n",
    "                        model_5_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_6: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_6_results['accuracy'],\n",
    "                        model_6_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_5 & model_6 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_3_results.csv\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # finished the three tasks\n",
    "        final_results.append(results_per_repeat)\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(final_results[1:], columns=final_results[0]).set_index('repeats')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def full_run_for_ds_w_eop(rseeds,\n",
    "                    privileged_groups,\n",
    "                    unprivileged_groups,\n",
    "                    dataset_orig,\n",
    "                    estimators, \n",
    "                    parameters, \n",
    "                    cv_fold_num, \n",
    "                    save_results=False):\n",
    "    \"\"\"for a specified dataset, runs the entirety of the 3 tasks' pipelines and repeat for 5 times,\n",
    "       each time with an initial split controlled by the random seed from the rseeds list,\n",
    "       the task 2 method has been switched from preprocessing reweighing to postprocessing Equalized Odds\n",
    "    \"\"\"\n",
    "    \n",
    "    final_results = [['repeats',\n",
    "                      'model_1_accuracy',\n",
    "                      'model_1_eq_opp_diff',\n",
    "                      'model_2_accuracy',\n",
    "                      'model_2_eq_opp_diff',\n",
    "                      'model_3_accuracy',\n",
    "                      'model_3_eq_opp_diff',\n",
    "                      'model_4_accuracy',\n",
    "                      'model_4_eq_opp_diff',\n",
    "                      'model_5_accuracy',\n",
    "                      'model_5_eq_opp_diff',\n",
    "                      'model_6_accuracy',\n",
    "                      'model_6_eq_opp_diff']]\n",
    "\n",
    "    for i, rseed in enumerate(rseeds):\n",
    "        repeat_num = i + 1\n",
    "\n",
    "        results_per_repeat = [f\"Repeat_{repeat_num}\"]\n",
    "\n",
    "        # set the random seed\n",
    "        np.random.seed(rseed)\n",
    "\n",
    "        # task 1: first shuffle and split the whole dataset into train-test\n",
    "        train, test = dataset_orig.split([0.7], shuffle=True)\n",
    "        print('\\nTotal number of training examples:', len(train.features))\n",
    "\n",
    "\n",
    "        \n",
    "        task_1_results_dataframes = cross_validate_search(train, \n",
    "                      estimators, \n",
    "                      parameters, \n",
    "                      cv_fold_num, \n",
    "                      unprivileged_groups, \n",
    "                      privileged_groups,\n",
    "                      visualization=False)\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the task 1 cv results are:\\n\\n\", task_1_results_dataframes[0], '\\n\\n', task_1_results_dataframes[1])\n",
    "        \n",
    "        \n",
    "        # test the selected models on held-out test dataset\n",
    "        # best model based on accuracy(model 1):\n",
    "        acc_model = task_1_results_dataframes[0].reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_1 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_1 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # best model based on fairness(model 2):\n",
    "        fair_model = task_1_results_dataframes[1].reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_2 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_2 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # train the selected models\n",
    "        model_1_trained = train_model(model_1, train)\n",
    "        model_2_trained = train_model(model_2, train)\n",
    "\n",
    "        model_1_results = evaluate(model_1_trained, test, unprivileged_groups, privileged_groups)\n",
    "        model_2_results = evaluate(model_2_trained, test, unprivileged_groups, privileged_groups)\n",
    "\n",
    "        results_per_repeat.append(model_1_results['accuracy'])\n",
    "        results_per_repeat.append(model_1_results['eq_opp_diff'])\n",
    "        results_per_repeat.append(model_2_results['accuracy'])\n",
    "        results_per_repeat.append(model_2_results['eq_opp_diff'])\n",
    "\n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_1: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_1_results['accuracy'],\n",
    "                        model_1_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_2: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_2_results['accuracy'],\n",
    "                        model_2_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_1 & model_2 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_1_results.csv\")\n",
    "\n",
    "\n",
    "        # task 2: with Equalized Odds\n",
    "        task_2_results_dataframes = cross_validate_search_w_eop(train, \n",
    "                       estimators, \n",
    "                       parameters, \n",
    "                       cv_fold_num, \n",
    "                       unprivileged_groups, \n",
    "                       privileged_groups,\n",
    "                       visualization=False)\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the task 2 cv results are:\\n\\n\", task_2_results_dataframes[0], '\\n\\n', task_2_results_dataframes[1])\n",
    "        \n",
    "        # test the selected models from task 2 on held-out test dataset\n",
    "        # best model based on accuracy(model 3):\n",
    "        acc_model = task_2_results_dataframes[0].reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_3 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_3 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # best model based on fairness(model 4):\n",
    "        fair_model = task_2_results_dataframes[1].reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_4 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_4 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # train the selected models\n",
    "        model_3_trained = train_model(model_3, train)\n",
    "        model_4_trained = train_model(model_4, train)\n",
    "\n",
    "        model_3_results = evaluate_w_eop(model_3_trained, test, unprivileged_groups, privileged_groups)\n",
    "        model_4_results = evaluate_w_eop(model_4_trained, test, unprivileged_groups, privileged_groups)\n",
    "\n",
    "        results_per_repeat.append(model_3_results['accuracy'])\n",
    "        results_per_repeat.append(model_3_results['eq_opp_diff'])\n",
    "        results_per_repeat.append(model_4_results['accuracy'])\n",
    "        results_per_repeat.append(model_4_results['eq_opp_diff'])\n",
    "\n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_3: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_3_results['accuracy'],\n",
    "                        model_3_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_4: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_4_results['accuracy'],\n",
    "                        model_4_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_3 & model_4 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_2_results.csv\")\n",
    "        \n",
    "        # Task 3: model selection based on both accuracy and fairness\n",
    "        # first for the fairness method(reweighing from task2) based model(model 5)\n",
    "        combined_fairness_df = task_2_results_dataframes[0]\n",
    "        combined_fairness_df['combinedmetric'] = combined_metric(combined_fairness_df['accuracy'], \n",
    "                                                                 abs(combined_fairness_df['eq_opp_diff']))\n",
    "        combined_fairness_df = combined_fairness_df.sort_values(by='combinedmetric', ascending=False)\n",
    "        \n",
    "        # select the model 5 as the best model based on combinedmetric result using task 2 method\n",
    "        fair_model = combined_fairness_df.reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_5 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_5 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "        \n",
    "        # train and test the model 5\n",
    "        model_5_trained = train_model(model_5, train)\n",
    "        model_5_results = evaluate_w_eop(model_5_trained, test, unprivileged_groups, privileged_groups)\n",
    "        \n",
    "        results_per_repeat.append(model_5_results['accuracy'])\n",
    "        results_per_repeat.append(model_5_results['eq_opp_diff'])\n",
    "        \n",
    "        # now for the standard method model(from task 1)\n",
    "        combined_standard_df = task_1_results_dataframes[0]\n",
    "        combined_standard_df['combinedmetric'] = combined_metric(combined_standard_df['accuracy'], \n",
    "                                                                 abs(combined_standard_df['eq_opp_diff']))\n",
    "        combined_standard_df = combined_standard_df.sort_values(by='combinedmetric', ascending=False)\n",
    "        \n",
    "        # select the model 6 as the best model based on combinedmetric result using task 1 method\n",
    "        acc_model = combined_standard_df.reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_6 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_6 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "        \n",
    "        # train and test the model 6\n",
    "        model_6_trained = train_model(model_6, train)\n",
    "        model_6_results = evaluate(model_6_trained, test, unprivileged_groups, privileged_groups)\n",
    "        \n",
    "        results_per_repeat.append(model_6_results['accuracy'])\n",
    "        results_per_repeat.append(model_6_results['eq_opp_diff'])\n",
    "        \n",
    "        print(combined_fairness_df)\n",
    "        print()\n",
    "        print(combined_standard_df)\n",
    "        \n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_5: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_5_results['accuracy'],\n",
    "                        model_5_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_6: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_6_results['accuracy'],\n",
    "                        model_6_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_5 & model_6 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_3_results.csv\")\n",
    "        \n",
    "        \n",
    "    \n",
    "        # finished the three tasks\n",
    "        final_results.append(results_per_repeat)\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(final_results[1:], columns=final_results[0]).set_index('repeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI3ztLtzQp2u"
   },
   "outputs": [],
   "source": [
    "# select dataset\n",
    "dataset_used = \"adult\"\n",
    "# dataset_used = \"german\"\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "    dataset_orig = load_preproc_data_adult(['sex'])\n",
    "elif dataset_used == \"german\":\n",
    "    privileged_groups = [{'age': 1}]\n",
    "    unprivileged_groups = [{'age': 0}]\n",
    "    dataset_orig = load_preproc_data_german(['age'])\n",
    "\n",
    "\n",
    "# define all the model types to be used and specify the hyperparam(C)'s search range\n",
    "estimators = {'lr':LogisticRegression, 'svm':svm.LinearSVC}\n",
    "\n",
    "# C_search_range = np.arange(0.1, 0.5, 0.1)\n",
    "# C_search_range = [0.001, 0.01, 0.1, 1]\n",
    "C_search_range = np.logspace(-4, 1, 6)\n",
    "\n",
    "parameters = OrderedDict(\n",
    "    [('lr', {'lr_C':C_search_range}),\n",
    "    ('svm', {'svm_C':C_search_range})]\n",
    "    )\n",
    "\n",
    "# rseeds = [0, 99, 199, 299, 999]\n",
    "rseeds = [1, 222, 444, 888, 248]  # final3\n",
    "# rseeds = [4, 5, 6, 7, 8] # final4_german\n",
    "ds_results = full_run_for_ds(rseeds,\n",
    "                             privileged_groups,\n",
    "                             unprivileged_groups,\n",
    "                             dataset_orig,\n",
    "                             estimators, \n",
    "                             parameters, \n",
    "                             4, \n",
    "                             save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_results.to_csv('final3_full.csv')\n",
    "print(ds_results)\n",
    "mean = pd.Series(ds_results.mean(), name='mean')\n",
    "std = pd.Series(ds_results.std(), name='std')\n",
    "final_stats = pd.concat([mean.to_frame(), std.to_frame()], axis=1).transpose()\n",
    "final_stats.to_csv('result3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI3ztLtzQp2u"
   },
   "outputs": [],
   "source": [
    "# select dataset\n",
    "dataset_used = \"adult\"\n",
    "# dataset_used = \"german\"\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "    dataset_orig = load_preproc_data_adult(['sex'])\n",
    "elif dataset_used == \"german\":\n",
    "    privileged_groups = [{'age': 1}]\n",
    "    unprivileged_groups = [{'age': 0}]\n",
    "    dataset_orig = load_preproc_data_german(['age'])\n",
    "\n",
    "\n",
    "# define all the model types to be used and specify the hyperparam(C)'s search range\n",
    "estimators = {'lr':LogisticRegression, 'svm':svm.LinearSVC}\n",
    "\n",
    "# C_search_range = np.arange(0.1, 0.5, 0.1)\n",
    "# C_search_range = [0.001, 0.01, 0.1, 1]\n",
    "C_search_range = np.logspace(-4, 1, 6)\n",
    "\n",
    "parameters = OrderedDict(\n",
    "    [('lr', {'lr_C':C_search_range}),\n",
    "    ('svm', {'svm_C':C_search_range})]\n",
    "    )\n",
    "\n",
    "# rseeds = [0, 99, 199, 299, 999]\n",
    "rseeds = [1, 222, 444, 888, 248]  # final3\n",
    "# rseeds = [4, 5, 6, 7, 8] # final4_german\n",
    "ds_eop_results = full_run_for_ds_w_eop(rseeds,\n",
    "                             privileged_groups,\n",
    "                             unprivileged_groups,\n",
    "                             dataset_orig,\n",
    "                             estimators, \n",
    "                             parameters, \n",
    "                             4, \n",
    "                             save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_results.to_csv('final4_german.csv')\n",
    "ds_eop_results.to_csv('final3_eop4.csv')\n",
    "print(ds_eop_results)\n",
    "mean = pd.Series(ds_eop_results.mean(), name='mean')\n",
    "std = pd.Series(ds_eop_results.std(), name='std')\n",
    "final_stats = pd.concat([mean.to_frame(), std.to_frame()], axis=1).transpose()\n",
    "final_stats.to_csv('final3_eop.csv')\n",
    "final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def full_run_for_ds_nsf(rseeds,\n",
    "                    privileged_groups,\n",
    "                    unprivileged_groups,\n",
    "                    sensitive_attr,\n",
    "                    dataset_orig,\n",
    "                    estimators, \n",
    "                    parameters, \n",
    "                    cv_fold_num, \n",
    "                    save_results=False):\n",
    "    \"\"\"for a specified dataset, runs the entirety of the 3 tasks' pipelines and repeat for 5 times,\n",
    "       each time with an initial split controlled by the random seed from the rseeds list,\n",
    "       this strictly excludes sensitive attribute from input feature X (nsf: no sensitive feature)\n",
    "    \"\"\"\n",
    "    \n",
    "    final_results = [['repeats',\n",
    "                      'model_1_accuracy',\n",
    "                      'model_1_eq_opp_diff',\n",
    "                      'model_2_accuracy',\n",
    "                      'model_2_eq_opp_diff',\n",
    "                      'model_3_accuracy',\n",
    "                      'model_3_eq_opp_diff',\n",
    "                      'model_4_accuracy',\n",
    "                      'model_4_eq_opp_diff',\n",
    "                      'model_5_accuracy',\n",
    "                      'model_5_eq_opp_diff',\n",
    "                      'model_6_accuracy',\n",
    "                      'model_6_eq_opp_diff']]\n",
    "\n",
    "    for i, rseed in enumerate(rseeds):\n",
    "        repeat_num = i + 1\n",
    "\n",
    "        results_per_repeat = [f\"Repeat_{repeat_num}\"]\n",
    "\n",
    "        # set the random seed\n",
    "        np.random.seed(rseed)\n",
    "\n",
    "        # task 1: first shuffle and split the whole dataset into train-test\n",
    "        train, test = dataset_orig.split([0.7], shuffle=True)\n",
    "        print('\\nTotal number of training examples:', len(train.features))\n",
    "\n",
    "\n",
    "        \n",
    "        task_1_results_dataframes = cross_validate_search(train, \n",
    "                      estimators, \n",
    "                      parameters, \n",
    "                      cv_fold_num, \n",
    "                      unprivileged_groups, \n",
    "                      privileged_groups,\n",
    "                      sensitive_attr=sensitive_attr,\n",
    "                      visualization=False)\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the task 1 cv results are:\\n\\n\", task_1_results_dataframes[0], '\\n\\n', task_1_results_dataframes[1])\n",
    "        \n",
    "        \n",
    "        # test the selected models on held-out test dataset\n",
    "        # best model based on accuracy(model 1):\n",
    "        acc_model = task_1_results_dataframes[0].reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_1 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_1 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # best model based on fairness(model 2):\n",
    "        fair_model = task_1_results_dataframes[1].reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_2 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_2 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # train the selected models\n",
    "        model_1_trained = train_model(model_1, train, sensitive_attr=sensitive_attr)\n",
    "        model_2_trained = train_model(model_2, train, sensitive_attr=sensitive_attr)\n",
    "\n",
    "        model_1_results = evaluate(model_1_trained, test, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "        model_2_results = evaluate(model_2_trained, test, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "\n",
    "        results_per_repeat.append(model_1_results['accuracy'])\n",
    "        results_per_repeat.append(model_1_results['eq_opp_diff'])\n",
    "        results_per_repeat.append(model_2_results['accuracy'])\n",
    "        results_per_repeat.append(model_2_results['eq_opp_diff'])\n",
    "\n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_1: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_1_results['accuracy'],\n",
    "                        model_1_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_2: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_2_results['accuracy'],\n",
    "                        model_2_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_1 & model_2 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_1_results.csv\")\n",
    "\n",
    "\n",
    "        # task 2: with reweighing\n",
    "        task_2_results_dataframes = cross_validate_search_w_reweigh(train, \n",
    "                       estimators, \n",
    "                       parameters, \n",
    "                       cv_fold_num, \n",
    "                       unprivileged_groups, \n",
    "                       privileged_groups,\n",
    "                       sensitive_attr=sensitive_attr,\n",
    "                       visualization=False)\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the task 2 cv results are:\\n\\n\", task_2_results_dataframes[0], '\\n\\n', task_2_results_dataframes[1])\n",
    "        \n",
    "        # test the selected models from task 2 on held-out test dataset\n",
    "        # best model based on accuracy(model 3):\n",
    "        acc_model = task_2_results_dataframes[0].reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_3 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_3 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # best model based on fairness(model 4):\n",
    "        fair_model = task_2_results_dataframes[1].reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_4 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_4 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "\n",
    "        # train the selected models\n",
    "        model_3_trained = train_model_w_reweigh(model_3, train, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "        model_4_trained = train_model_w_reweigh(model_4, train, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "\n",
    "        model_3_results = evaluate_w_reweigh(model_3_trained, test, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "        model_4_results = evaluate_w_reweigh(model_4_trained, test, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "\n",
    "        results_per_repeat.append(model_3_results['accuracy'])\n",
    "        results_per_repeat.append(model_3_results['eq_opp_diff'])\n",
    "        results_per_repeat.append(model_4_results['accuracy'])\n",
    "        results_per_repeat.append(model_4_results['eq_opp_diff'])\n",
    "\n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_3: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_3_results['accuracy'],\n",
    "                        model_3_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_4: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_4_results['accuracy'],\n",
    "                        model_4_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_3 & model_4 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_2_results.csv\")\n",
    "          \n",
    "        # Task 3: model selection based on both accuracy and fairness\n",
    "    \n",
    "        # first for the fairness method(reweighing from task2) based model(model 5)\n",
    "        combined_fairness_df = task_2_results_dataframes[0]\n",
    "        combined_fairness_df['combinedmetric'] = combined_metric(combined_fairness_df['accuracy'], \n",
    "                                                                 abs(combined_fairness_df['eq_opp_diff']))\n",
    "        combined_fairness_df = combined_fairness_df.sort_values(by='combinedmetric', ascending=False)\n",
    "        \n",
    "        # select the model 5 as the best model based on combinedmetric result using task 2 method\n",
    "        fair_model = combined_fairness_df.reset_index().iloc[0][0].split('_')\n",
    "        if fair_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            fair_model_type = 'lr'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_5 = LogisticRegression(C=fair_C_best, solver=SOLVER, random_state=1)\n",
    "        elif fair_model[0] == 'svm':\n",
    "            # svm model\n",
    "            fair_model_type = 'svm'\n",
    "            fair_C_best = float(fair_model[-1])\n",
    "            model_5 = svm.LinearSVC(C=fair_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "        \n",
    "        # train and test the model 5\n",
    "        model_5_trained = train_model_w_reweigh(model_5, train, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "        model_5_results = evaluate_w_reweigh(model_5_trained, test, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "        \n",
    "        results_per_repeat.append(model_5_results['accuracy'])\n",
    "        results_per_repeat.append(model_5_results['eq_opp_diff'])\n",
    "        \n",
    "        # now for the standard method model(from task 1)\n",
    "        combined_standard_df = task_1_results_dataframes[0]\n",
    "        combined_standard_df['combinedmetric'] =combined_metric(combined_standard_df['accuracy'], \n",
    "                                                                 abs(combined_standard_df['eq_opp_diff']))\n",
    "        combined_standard_df = combined_standard_df.sort_values(by='combinedmetric', ascending=False)\n",
    "        \n",
    "        # select the model 6 as the best model based on combinedmetric result using task 1 method\n",
    "        acc_model = combined_standard_df.reset_index().iloc[0][0].split('_')\n",
    "        if acc_model[0] == 'lr':\n",
    "            # logistic regression model\n",
    "            acc_model_type = 'lr'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_6 = LogisticRegression(C=acc_C_best, solver=SOLVER, random_state=1)\n",
    "        elif acc_model[0] == 'svm':\n",
    "            # svm model\n",
    "            acc_model_type = 'svm'\n",
    "            acc_C_best = float(acc_model[-1])\n",
    "            model_6 = svm.LinearSVC(C=acc_C_best, max_iter=MAX_ITER, random_state=1)\n",
    "        \n",
    "        # train and test the model 6\n",
    "        model_6_trained = train_model(model_6, train, sensitive_attr=sensitive_attr)\n",
    "        model_6_results = evaluate(model_6_trained, test, unprivileged_groups, privileged_groups, sensitive_attr=sensitive_attr)\n",
    "        \n",
    "        results_per_repeat.append(model_6_results['accuracy'])\n",
    "        results_per_repeat.append(model_6_results['eq_opp_diff'])\n",
    "        \n",
    "        print(combined_fairness_df)\n",
    "        print()\n",
    "        print(combined_standard_df)\n",
    "        \n",
    "        results = [['model', 'accuracy', 'eq_opp_diff']]\n",
    "        # test the selected models\n",
    "        results.append([f'model_5: {fair_model_type}_C_{fair_C_best}',\n",
    "                        model_5_results['accuracy'],\n",
    "                        model_5_results['eq_opp_diff']\n",
    "                      ])\n",
    "        results.append([f'model_6: {acc_model_type}_C_{acc_C_best}',\n",
    "                        model_6_results['accuracy'],\n",
    "                        model_6_results['eq_opp_diff']\n",
    "                      ])\n",
    "\n",
    "        print(f\"\\nAt repeat number {repeat_num}, the test results for model_5 & model_6 are: \\n\\n\", pd.DataFrame(results[1:], columns=results[0]).set_index('model'))\n",
    "        if save_results is True:\n",
    "            pd.DataFrame(results[1:], columns=results[0]).set_index('model').to_csv(f\"repeat_{repeat_num}_task_3_results.csv\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # finished the three tasks\n",
    "        final_results.append(results_per_repeat)\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(final_results[1:], columns=final_results[0]).set_index('repeats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of training examples: 700\n",
      "\n",
      "At repeat number 1, the task 1 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.701429     0.007426\n",
      "lr_C_1.0      0.701429     0.007426\n",
      "lr_C_10.0     0.701429     0.007426\n",
      "svm_C_0.01    0.701429     0.007426\n",
      "svm_C_0.1     0.701429     0.007426\n",
      "svm_C_1.0     0.701429     0.007426\n",
      "svm_C_10.0    0.701429     0.007426\n",
      "lr_C_0.01     0.681429    -0.050671\n",
      "svm_C_0.001   0.681429    -0.050671\n",
      "lr_C_0.0001   0.662857    -0.088073\n",
      "lr_C_0.001    0.662857    -0.088073\n",
      "svm_C_0.0001  0.662857    -0.088073 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.701429     0.007426\n",
      "lr_C_1.0      0.701429     0.007426\n",
      "lr_C_10.0     0.701429     0.007426\n",
      "svm_C_0.01    0.701429     0.007426\n",
      "svm_C_0.1     0.701429     0.007426\n",
      "svm_C_1.0     0.701429     0.007426\n",
      "svm_C_10.0    0.701429     0.007426\n",
      "lr_C_0.01     0.681429    -0.050671\n",
      "svm_C_0.001   0.681429    -0.050671\n",
      "lr_C_0.0001   0.662857    -0.088073\n",
      "lr_C_0.001    0.662857    -0.088073\n",
      "svm_C_0.0001  0.662857    -0.088073\n",
      "\n",
      "At repeat number 1, the test results for model_1 & model_2 are: \n",
      "\n",
      "                    accuracy  eq_opp_diff\n",
      "model                                   \n",
      "model_1: lr_C_0.1      0.68     -0.03441\n",
      "model_2: lr_C_0.1      0.68     -0.03441\n",
      "\n",
      "At repeat number 1, the task 2 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.01     0.700000     0.009901\n",
      "lr_C_0.1      0.700000     0.009901\n",
      "lr_C_1.0      0.700000     0.009901\n",
      "lr_C_10.0     0.700000     0.009901\n",
      "svm_C_0.001   0.700000     0.009901\n",
      "svm_C_0.01    0.700000     0.009901\n",
      "svm_C_0.1     0.700000     0.009901\n",
      "svm_C_1.0     0.700000     0.009901\n",
      "svm_C_10.0    0.700000     0.009901\n",
      "lr_C_0.0001   0.661429    -0.085598\n",
      "lr_C_0.001    0.661429    -0.085598\n",
      "svm_C_0.0001  0.661429    -0.085598 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.01     0.700000     0.009901\n",
      "lr_C_0.1      0.700000     0.009901\n",
      "lr_C_1.0      0.700000     0.009901\n",
      "lr_C_10.0     0.700000     0.009901\n",
      "svm_C_0.001   0.700000     0.009901\n",
      "svm_C_0.01    0.700000     0.009901\n",
      "svm_C_0.1     0.700000     0.009901\n",
      "svm_C_1.0     0.700000     0.009901\n",
      "svm_C_10.0    0.700000     0.009901\n",
      "lr_C_0.0001   0.661429    -0.085598\n",
      "lr_C_0.001    0.661429    -0.085598\n",
      "svm_C_0.0001  0.661429    -0.085598\n",
      "\n",
      "At repeat number 1, the test results for model_3 & model_4 are: \n",
      "\n",
      "                     accuracy  eq_opp_diff\n",
      "model                                    \n",
      "model_3: lr_C_0.01      0.68     -0.03441\n",
      "model_4: lr_C_0.01      0.68     -0.03441\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "lr_C_0.01     0.700000     0.009901        0.685149\n",
      "lr_C_0.1      0.700000     0.009901        0.685149\n",
      "lr_C_1.0      0.700000     0.009901        0.685149\n",
      "lr_C_10.0     0.700000     0.009901        0.685149\n",
      "svm_C_0.001   0.700000     0.009901        0.685149\n",
      "svm_C_0.01    0.700000     0.009901        0.685149\n",
      "svm_C_0.1     0.700000     0.009901        0.685149\n",
      "svm_C_1.0     0.700000     0.009901        0.685149\n",
      "svm_C_10.0    0.700000     0.009901        0.685149\n",
      "lr_C_0.0001   0.661429    -0.085598        0.533031\n",
      "lr_C_0.001    0.661429    -0.085598        0.533031\n",
      "svm_C_0.0001  0.661429    -0.085598        0.533031\n",
      "\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "lr_C_0.1      0.701429     0.007426        0.690290\n",
      "lr_C_1.0      0.701429     0.007426        0.690290\n",
      "lr_C_10.0     0.701429     0.007426        0.690290\n",
      "svm_C_0.01    0.701429     0.007426        0.690290\n",
      "svm_C_0.1     0.701429     0.007426        0.690290\n",
      "svm_C_1.0     0.701429     0.007426        0.690290\n",
      "svm_C_10.0    0.701429     0.007426        0.690290\n",
      "lr_C_0.01     0.681429    -0.050671        0.605422\n",
      "svm_C_0.001   0.681429    -0.050671        0.605422\n",
      "lr_C_0.0001   0.662857    -0.088073        0.530747\n",
      "lr_C_0.001    0.662857    -0.088073        0.530747\n",
      "svm_C_0.0001  0.662857    -0.088073        0.530747\n",
      "\n",
      "At repeat number 1, the test results for model_5 & model_6 are: \n",
      "\n",
      "                     accuracy  eq_opp_diff\n",
      "model                                    \n",
      "model_5: lr_C_0.01      0.68     -0.03441\n",
      "model_6: lr_C_0.1       0.68     -0.03441\n",
      "\n",
      "Total number of training examples: 700\n",
      "\n",
      "At repeat number 2, the task 1 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.01    0.717143     0.000000\n",
      "svm_C_0.1     0.717143     0.000000\n",
      "svm_C_1.0     0.717143     0.000000\n",
      "svm_C_10.0    0.717143     0.000000\n",
      "lr_C_0.01     0.715714    -0.013694\n",
      "lr_C_0.1      0.715714    -0.013694\n",
      "lr_C_1.0      0.715714    -0.013694\n",
      "lr_C_10.0     0.715714    -0.013694\n",
      "svm_C_0.001   0.715714    -0.013694\n",
      "lr_C_0.001    0.671429    -0.193188\n",
      "svm_C_0.0001  0.670000    -0.189089\n",
      "lr_C_0.0001   0.665714    -0.201507 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.01    0.717143     0.000000\n",
      "svm_C_0.1     0.717143     0.000000\n",
      "svm_C_1.0     0.717143     0.000000\n",
      "svm_C_10.0    0.717143     0.000000\n",
      "lr_C_0.01     0.715714    -0.013694\n",
      "lr_C_0.1      0.715714    -0.013694\n",
      "lr_C_1.0      0.715714    -0.013694\n",
      "lr_C_10.0     0.715714    -0.013694\n",
      "svm_C_0.001   0.715714    -0.013694\n",
      "svm_C_0.0001  0.670000    -0.189089\n",
      "lr_C_0.001    0.671429    -0.193188\n",
      "lr_C_0.0001   0.665714    -0.201507\n",
      "\n",
      "At repeat number 2, the test results for model_1 & model_2 are: \n",
      "\n",
      "                      accuracy  eq_opp_diff\n",
      "model                                     \n",
      "model_1: svm_C_0.01  0.663333          0.0\n",
      "model_2: svm_C_0.01  0.663333          0.0\n",
      "\n",
      "At repeat number 2, the task 2 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.717143     0.000000\n",
      "lr_C_1.0      0.717143     0.000000\n",
      "lr_C_10.0     0.717143     0.000000\n",
      "svm_C_0.01    0.717143     0.000000\n",
      "svm_C_0.1     0.717143     0.000000\n",
      "svm_C_1.0     0.717143     0.000000\n",
      "svm_C_10.0    0.717143     0.000000\n",
      "lr_C_0.01     0.712857    -0.013694\n",
      "svm_C_0.001   0.712857    -0.013694\n",
      "lr_C_0.001    0.705714    -0.123588\n",
      "svm_C_0.0001  0.688571    -0.123019\n",
      "lr_C_0.0001   0.670000    -0.189089 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.717143     0.000000\n",
      "lr_C_1.0      0.717143     0.000000\n",
      "lr_C_10.0     0.717143     0.000000\n",
      "svm_C_0.01    0.717143     0.000000\n",
      "svm_C_0.1     0.717143     0.000000\n",
      "svm_C_1.0     0.717143     0.000000\n",
      "svm_C_10.0    0.717143     0.000000\n",
      "lr_C_0.01     0.712857    -0.013694\n",
      "svm_C_0.001   0.712857    -0.013694\n",
      "svm_C_0.0001  0.688571    -0.123019\n",
      "lr_C_0.001    0.705714    -0.123588\n",
      "lr_C_0.0001   0.670000    -0.189089\n",
      "\n",
      "At repeat number 2, the test results for model_3 & model_4 are: \n",
      "\n",
      "                    accuracy  eq_opp_diff\n",
      "model                                   \n",
      "model_3: lr_C_0.1  0.663333          0.0\n",
      "model_4: lr_C_0.1  0.663333          0.0\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "lr_C_0.1      0.717143     0.000000        0.717143\n",
      "lr_C_1.0      0.717143     0.000000        0.717143\n",
      "lr_C_10.0     0.717143     0.000000        0.717143\n",
      "svm_C_0.01    0.717143     0.000000        0.717143\n",
      "svm_C_0.1     0.717143     0.000000        0.717143\n",
      "svm_C_1.0     0.717143     0.000000        0.717143\n",
      "svm_C_10.0    0.717143     0.000000        0.717143\n",
      "lr_C_0.01     0.712857    -0.013694        0.692316\n",
      "svm_C_0.001   0.712857    -0.013694        0.692316\n",
      "lr_C_0.001    0.705714    -0.123588        0.520333\n",
      "svm_C_0.0001  0.688571    -0.123019        0.504042\n",
      "lr_C_0.0001   0.670000    -0.189089        0.386366\n",
      "\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "svm_C_0.01    0.717143     0.000000        0.717143\n",
      "svm_C_0.1     0.717143     0.000000        0.717143\n",
      "svm_C_1.0     0.717143     0.000000        0.717143\n",
      "svm_C_10.0    0.717143     0.000000        0.717143\n",
      "lr_C_0.01     0.715714    -0.013694        0.695174\n",
      "lr_C_0.1      0.715714    -0.013694        0.695174\n",
      "lr_C_1.0      0.715714    -0.013694        0.695174\n",
      "lr_C_10.0     0.715714    -0.013694        0.695174\n",
      "svm_C_0.001   0.715714    -0.013694        0.695174\n",
      "svm_C_0.0001  0.670000    -0.189089        0.386366\n",
      "lr_C_0.001    0.671429    -0.193188        0.381647\n",
      "lr_C_0.0001   0.665714    -0.201507        0.363453\n",
      "\n",
      "At repeat number 2, the test results for model_5 & model_6 are: \n",
      "\n",
      "                      accuracy  eq_opp_diff\n",
      "model                                     \n",
      "model_5: lr_C_0.1    0.663333          0.0\n",
      "model_6: svm_C_0.01  0.663333          0.0\n",
      "\n",
      "Total number of training examples: 700\n",
      "\n",
      "At repeat number 3, the task 1 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.01    0.694286    -0.024208\n",
      "svm_C_0.1     0.694286    -0.024208\n",
      "svm_C_1.0     0.694286    -0.024208\n",
      "svm_C_10.0    0.694286    -0.024208\n",
      "lr_C_0.01     0.687143    -0.101269\n",
      "lr_C_0.1      0.687143    -0.103673\n",
      "lr_C_1.0      0.687143    -0.103673\n",
      "lr_C_10.0     0.687143    -0.103673\n",
      "svm_C_0.001   0.687143    -0.101269\n",
      "lr_C_0.0001   0.655714    -0.179962\n",
      "lr_C_0.001    0.648571    -0.155498\n",
      "svm_C_0.0001  0.648571    -0.155498 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.01    0.694286    -0.024208\n",
      "svm_C_0.1     0.694286    -0.024208\n",
      "svm_C_1.0     0.694286    -0.024208\n",
      "svm_C_10.0    0.694286    -0.024208\n",
      "lr_C_0.01     0.687143    -0.101269\n",
      "svm_C_0.001   0.687143    -0.101269\n",
      "lr_C_0.1      0.687143    -0.103673\n",
      "lr_C_1.0      0.687143    -0.103673\n",
      "lr_C_10.0     0.687143    -0.103673\n",
      "lr_C_0.001    0.648571    -0.155498\n",
      "svm_C_0.0001  0.648571    -0.155498\n",
      "lr_C_0.0001   0.655714    -0.179962\n",
      "\n",
      "At repeat number 3, the test results for model_1 & model_2 are: \n",
      "\n",
      "                      accuracy  eq_opp_diff\n",
      "model                                     \n",
      "model_1: svm_C_0.01  0.696667    -0.020502\n",
      "model_2: svm_C_0.01  0.696667    -0.020502\n",
      "\n",
      "At repeat number 3, the task 2 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.01    0.700000     0.004902\n",
      "svm_C_0.1     0.700000     0.004902\n",
      "svm_C_1.0     0.700000     0.004902\n",
      "svm_C_10.0    0.700000     0.004902\n",
      "lr_C_0.01     0.691429    -0.016485\n",
      "svm_C_0.001   0.691429    -0.016485\n",
      "lr_C_0.1      0.687143    -0.067652\n",
      "lr_C_1.0      0.687143    -0.067652\n",
      "lr_C_10.0     0.687143    -0.067652\n",
      "lr_C_0.001    0.650000    -0.170783\n",
      "svm_C_0.0001  0.650000    -0.170783\n",
      "lr_C_0.0001   0.645714    -0.150179 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.01    0.700000     0.004902\n",
      "svm_C_0.1     0.700000     0.004902\n",
      "svm_C_1.0     0.700000     0.004902\n",
      "svm_C_10.0    0.700000     0.004902\n",
      "lr_C_0.01     0.691429    -0.016485\n",
      "svm_C_0.001   0.691429    -0.016485\n",
      "lr_C_0.1      0.687143    -0.067652\n",
      "lr_C_1.0      0.687143    -0.067652\n",
      "lr_C_10.0     0.687143    -0.067652\n",
      "lr_C_0.0001   0.645714    -0.150179\n",
      "lr_C_0.001    0.650000    -0.170783\n",
      "svm_C_0.0001  0.650000    -0.170783\n",
      "\n",
      "At repeat number 3, the test results for model_3 & model_4 are: \n",
      "\n",
      "                      accuracy  eq_opp_diff\n",
      "model                                     \n",
      "model_3: svm_C_0.01  0.696667    -0.020502\n",
      "model_4: svm_C_0.01  0.696667    -0.020502\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "svm_C_0.01    0.700000     0.004902        0.692647\n",
      "svm_C_0.1     0.700000     0.004902        0.692647\n",
      "svm_C_1.0     0.700000     0.004902        0.692647\n",
      "svm_C_10.0    0.700000     0.004902        0.692647\n",
      "lr_C_0.01     0.691429    -0.016485        0.666702\n",
      "svm_C_0.001   0.691429    -0.016485        0.666702\n",
      "lr_C_0.1      0.687143    -0.067652        0.585665\n",
      "lr_C_1.0      0.687143    -0.067652        0.585665\n",
      "lr_C_10.0     0.687143    -0.067652        0.585665\n",
      "lr_C_0.0001   0.645714    -0.150179        0.420446\n",
      "lr_C_0.001    0.650000    -0.170783        0.393825\n",
      "svm_C_0.0001  0.650000    -0.170783        0.393825\n",
      "\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "svm_C_0.01    0.694286    -0.024208        0.657974\n",
      "svm_C_0.1     0.694286    -0.024208        0.657974\n",
      "svm_C_1.0     0.694286    -0.024208        0.657974\n",
      "svm_C_10.0    0.694286    -0.024208        0.657974\n",
      "lr_C_0.01     0.687143    -0.101269        0.535239\n",
      "svm_C_0.001   0.687143    -0.101269        0.535239\n",
      "lr_C_0.1      0.687143    -0.103673        0.531633\n",
      "lr_C_1.0      0.687143    -0.103673        0.531633\n",
      "lr_C_10.0     0.687143    -0.103673        0.531633\n",
      "lr_C_0.001    0.648571    -0.155498        0.415324\n",
      "svm_C_0.0001  0.648571    -0.155498        0.415324\n",
      "lr_C_0.0001   0.655714    -0.179962        0.385772\n",
      "\n",
      "At repeat number 3, the test results for model_5 & model_6 are: \n",
      "\n",
      "                      accuracy  eq_opp_diff\n",
      "model                                     \n",
      "model_5: svm_C_0.01  0.696667    -0.020502\n",
      "model_6: svm_C_0.01  0.696667    -0.020502\n",
      "\n",
      "Total number of training examples: 700\n",
      "\n",
      "At repeat number 4, the task 1 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.711429    -0.001071\n",
      "lr_C_0.01     0.704286    -0.004177\n",
      "lr_C_1.0      0.702857    -0.007426\n",
      "lr_C_10.0     0.702857    -0.007426\n",
      "svm_C_0.01    0.702857     0.009164\n",
      "svm_C_0.1     0.702857     0.009164\n",
      "svm_C_1.0     0.702857     0.009164\n",
      "svm_C_10.0    0.702857     0.009164\n",
      "svm_C_0.001   0.697143    -0.045695\n",
      "lr_C_0.001    0.674286    -0.168301\n",
      "svm_C_0.0001  0.674286    -0.168301\n",
      "lr_C_0.0001   0.667143    -0.153972 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.711429    -0.001071\n",
      "lr_C_0.01     0.704286    -0.004177\n",
      "lr_C_1.0      0.702857    -0.007426\n",
      "lr_C_10.0     0.702857    -0.007426\n",
      "svm_C_0.01    0.702857     0.009164\n",
      "svm_C_0.1     0.702857     0.009164\n",
      "svm_C_1.0     0.702857     0.009164\n",
      "svm_C_10.0    0.702857     0.009164\n",
      "svm_C_0.001   0.697143    -0.045695\n",
      "lr_C_0.0001   0.667143    -0.153972\n",
      "lr_C_0.001    0.674286    -0.168301\n",
      "svm_C_0.0001  0.674286    -0.168301\n",
      "\n",
      "At repeat number 4, the test results for model_1 & model_2 are: \n",
      "\n",
      "                    accuracy  eq_opp_diff\n",
      "model                                   \n",
      "model_1: lr_C_0.1  0.706667          0.0\n",
      "model_2: lr_C_0.1  0.706667          0.0\n",
      "\n",
      "At repeat number 4, the task 2 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.704286     0.009164\n",
      "lr_C_1.0      0.704286     0.009164\n",
      "lr_C_10.0     0.704286     0.009164\n",
      "svm_C_0.01    0.704286     0.009164\n",
      "svm_C_0.1     0.704286     0.009164\n",
      "svm_C_1.0     0.704286     0.009164\n",
      "svm_C_10.0    0.704286     0.009164\n",
      "lr_C_0.01     0.700000    -0.021264\n",
      "svm_C_0.001   0.700000    -0.021264\n",
      "lr_C_0.001    0.675714    -0.168301\n",
      "svm_C_0.0001  0.674286    -0.165874\n",
      "lr_C_0.0001   0.671429    -0.158374 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.704286     0.009164\n",
      "lr_C_1.0      0.704286     0.009164\n",
      "lr_C_10.0     0.704286     0.009164\n",
      "svm_C_0.01    0.704286     0.009164\n",
      "svm_C_0.1     0.704286     0.009164\n",
      "svm_C_1.0     0.704286     0.009164\n",
      "svm_C_10.0    0.704286     0.009164\n",
      "lr_C_0.01     0.700000    -0.021264\n",
      "svm_C_0.001   0.700000    -0.021264\n",
      "lr_C_0.0001   0.671429    -0.158374\n",
      "svm_C_0.0001  0.674286    -0.165874\n",
      "lr_C_0.001    0.675714    -0.168301\n",
      "\n",
      "At repeat number 4, the test results for model_3 & model_4 are: \n",
      "\n",
      "                    accuracy  eq_opp_diff\n",
      "model                                   \n",
      "model_3: lr_C_0.1  0.703333     0.005848\n",
      "model_4: lr_C_0.1  0.703333     0.005848\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "lr_C_0.1      0.704286     0.009164        0.690539\n",
      "lr_C_1.0      0.704286     0.009164        0.690539\n",
      "lr_C_10.0     0.704286     0.009164        0.690539\n",
      "svm_C_0.01    0.704286     0.009164        0.690539\n",
      "svm_C_0.1     0.704286     0.009164        0.690539\n",
      "svm_C_1.0     0.704286     0.009164        0.690539\n",
      "svm_C_10.0    0.704286     0.009164        0.690539\n",
      "lr_C_0.01     0.700000    -0.021264        0.668104\n",
      "svm_C_0.001   0.700000    -0.021264        0.668104\n",
      "lr_C_0.0001   0.671429    -0.158374        0.433867\n",
      "svm_C_0.0001  0.674286    -0.165874        0.425475\n",
      "lr_C_0.001    0.675714    -0.168301        0.423262\n",
      "\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "lr_C_0.1      0.711429    -0.001071        0.709823\n",
      "lr_C_0.01     0.704286    -0.004177        0.698020\n",
      "lr_C_1.0      0.702857    -0.007426        0.691717\n",
      "lr_C_10.0     0.702857    -0.007426        0.691717\n",
      "svm_C_0.01    0.702857     0.009164        0.689110\n",
      "svm_C_0.1     0.702857     0.009164        0.689110\n",
      "svm_C_1.0     0.702857     0.009164        0.689110\n",
      "svm_C_10.0    0.702857     0.009164        0.689110\n",
      "svm_C_0.001   0.697143    -0.045695        0.628600\n",
      "lr_C_0.0001   0.667143    -0.153972        0.436185\n",
      "lr_C_0.001    0.674286    -0.168301        0.421834\n",
      "svm_C_0.0001  0.674286    -0.168301        0.421834\n",
      "\n",
      "At repeat number 4, the test results for model_5 & model_6 are: \n",
      "\n",
      "                    accuracy  eq_opp_diff\n",
      "model                                   \n",
      "model_5: lr_C_0.1  0.703333     0.005848\n",
      "model_6: lr_C_0.1  0.706667     0.000000\n",
      "\n",
      "Total number of training examples: 700\n",
      "\n",
      "At repeat number 5, the task 1 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.01     0.694286    -0.055186\n",
      "svm_C_0.001   0.694286    -0.055186\n",
      "lr_C_0.1      0.692857    -0.031790\n",
      "lr_C_1.0      0.692857    -0.031790\n",
      "lr_C_10.0     0.692857    -0.031790\n",
      "svm_C_0.01    0.688571    -0.036999\n",
      "svm_C_0.1     0.684286    -0.043943\n",
      "svm_C_1.0     0.684286    -0.043943\n",
      "svm_C_10.0    0.684286    -0.043943\n",
      "lr_C_0.0001   0.661429    -0.109400\n",
      "lr_C_0.001    0.660000    -0.107064\n",
      "svm_C_0.0001  0.660000    -0.107064 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.1      0.692857    -0.031790\n",
      "lr_C_1.0      0.692857    -0.031790\n",
      "lr_C_10.0     0.692857    -0.031790\n",
      "svm_C_0.01    0.688571    -0.036999\n",
      "svm_C_0.1     0.684286    -0.043943\n",
      "svm_C_1.0     0.684286    -0.043943\n",
      "svm_C_10.0    0.684286    -0.043943\n",
      "lr_C_0.01     0.694286    -0.055186\n",
      "svm_C_0.001   0.694286    -0.055186\n",
      "lr_C_0.001    0.660000    -0.107064\n",
      "svm_C_0.0001  0.660000    -0.107064\n",
      "lr_C_0.0001   0.661429    -0.109400\n",
      "\n",
      "At repeat number 5, the test results for model_1 & model_2 are: \n",
      "\n",
      "                     accuracy  eq_opp_diff\n",
      "model                                    \n",
      "model_1: lr_C_0.01  0.713333    -0.043459\n",
      "model_2: lr_C_0.1   0.716667    -0.008692\n",
      "\n",
      "At repeat number 5, the task 2 cv results are:\n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "lr_C_0.01     0.694286    -0.057790\n",
      "svm_C_0.001   0.694286    -0.057790\n",
      "svm_C_0.01    0.690000     0.002335\n",
      "lr_C_0.1      0.688571    -0.041339\n",
      "lr_C_1.0      0.688571    -0.041339\n",
      "lr_C_10.0     0.688571    -0.041339\n",
      "svm_C_0.1     0.688571    -0.000270\n",
      "svm_C_1.0     0.688571    -0.000270\n",
      "svm_C_10.0    0.688571    -0.000270\n",
      "lr_C_0.0001   0.652857    -0.086230\n",
      "lr_C_0.001    0.652857    -0.065397\n",
      "svm_C_0.0001  0.652857    -0.065397 \n",
      "\n",
      "               accuracy  eq_opp_diff\n",
      "model                              \n",
      "svm_C_0.1     0.688571    -0.000270\n",
      "svm_C_1.0     0.688571    -0.000270\n",
      "svm_C_10.0    0.688571    -0.000270\n",
      "svm_C_0.01    0.690000     0.002335\n",
      "lr_C_0.1      0.688571    -0.041339\n",
      "lr_C_1.0      0.688571    -0.041339\n",
      "lr_C_10.0     0.688571    -0.041339\n",
      "lr_C_0.01     0.694286    -0.057790\n",
      "svm_C_0.001   0.694286    -0.057790\n",
      "lr_C_0.001    0.652857    -0.065397\n",
      "svm_C_0.0001  0.652857    -0.065397\n",
      "lr_C_0.0001   0.652857    -0.086230\n",
      "\n",
      "At repeat number 5, the test results for model_3 & model_4 are: \n",
      "\n",
      "                     accuracy  eq_opp_diff\n",
      "model                                    \n",
      "model_3: lr_C_0.01  0.716667    -0.008692\n",
      "model_4: svm_C_0.1  0.713333    -0.008692\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "svm_C_0.1     0.688571    -0.000270        0.688167\n",
      "svm_C_1.0     0.688571    -0.000270        0.688167\n",
      "svm_C_10.0    0.688571    -0.000270        0.688167\n",
      "svm_C_0.01    0.690000     0.002335        0.686498\n",
      "lr_C_0.1      0.688571    -0.041339        0.626563\n",
      "lr_C_1.0      0.688571    -0.041339        0.626563\n",
      "lr_C_10.0     0.688571    -0.041339        0.626563\n",
      "lr_C_0.01     0.694286    -0.057790        0.607600\n",
      "svm_C_0.001   0.694286    -0.057790        0.607600\n",
      "lr_C_0.001    0.652857    -0.065397        0.554762\n",
      "svm_C_0.0001  0.652857    -0.065397        0.554762\n",
      "lr_C_0.0001   0.652857    -0.086230        0.523512\n",
      "\n",
      "              accuracy  eq_opp_diff  combinedmetric\n",
      "model                                              \n",
      "lr_C_0.1      0.692857    -0.031790        0.645172\n",
      "lr_C_1.0      0.692857    -0.031790        0.645172\n",
      "lr_C_10.0     0.692857    -0.031790        0.645172\n",
      "svm_C_0.01    0.688571    -0.036999        0.633073\n",
      "svm_C_0.1     0.684286    -0.043943        0.618371\n",
      "svm_C_1.0     0.684286    -0.043943        0.618371\n",
      "svm_C_10.0    0.684286    -0.043943        0.618371\n",
      "lr_C_0.01     0.694286    -0.055186        0.611506\n",
      "svm_C_0.001   0.694286    -0.055186        0.611506\n",
      "lr_C_0.001    0.660000    -0.107064        0.499405\n",
      "svm_C_0.0001  0.660000    -0.107064        0.499405\n",
      "lr_C_0.0001   0.661429    -0.109400        0.497329\n",
      "\n",
      "At repeat number 5, the test results for model_5 & model_6 are: \n",
      "\n",
      "                     accuracy  eq_opp_diff\n",
      "model                                    \n",
      "model_5: svm_C_0.1  0.713333    -0.008692\n",
      "model_6: lr_C_0.1   0.716667    -0.008692\n"
     ]
    }
   ],
   "source": [
    "# select dataset\n",
    "# dataset_used = \"adult\"\n",
    "dataset_used = \"german\"\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "    sensitive_attr = ['sex']\n",
    "    dataset_orig = load_preproc_data_adult(['sex'])\n",
    "elif dataset_used == \"german\":\n",
    "    privileged_groups = [{'age': 1}]\n",
    "    unprivileged_groups = [{'age': 0}]\n",
    "    sensitive_attr = ['age']\n",
    "    dataset_orig = load_preproc_data_german(['age'])\n",
    "\n",
    "\n",
    "# define all the model types to be used and specify the hyperparam(C)'s search range\n",
    "estimators = {'lr':LogisticRegression, 'svm':svm.LinearSVC}\n",
    "\n",
    "# C_search_range = np.arange(0.1, 0.5, 0.1)\n",
    "# C_search_range = [0.001, 0.01, 0.1, 1]\n",
    "C_search_range = np.logspace(-4, 1, 6)\n",
    "\n",
    "parameters = OrderedDict(\n",
    "    [('lr', {'lr_C':C_search_range}),\n",
    "    ('svm', {'svm_C':C_search_range})]\n",
    "    )\n",
    "\n",
    "# rseeds = [0, 99, 199, 299, 999]\n",
    "# rseeds = [1, 222, 444, 888, 248]  # final3\n",
    "rseeds = [4, 5, 6, 7, 8] # final4_german\n",
    "ds_nsf_results = full_run_for_ds_nsf(rseeds,\n",
    "                             privileged_groups,\n",
    "                             unprivileged_groups,\n",
    "                             sensitive_attr,\n",
    "                             dataset_orig,\n",
    "                             estimators, \n",
    "                             parameters, \n",
    "                             4, \n",
    "                             save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          model_1_accuracy  model_1_eq_opp_diff  model_2_accuracy  \\\n",
      "repeats                                                             \n",
      "Repeat_1          0.680000            -0.034410          0.680000   \n",
      "Repeat_2          0.663333             0.000000          0.663333   \n",
      "Repeat_3          0.696667            -0.020502          0.696667   \n",
      "Repeat_4          0.706667             0.000000          0.706667   \n",
      "Repeat_5          0.713333            -0.043459          0.716667   \n",
      "\n",
      "          model_2_eq_opp_diff  model_3_accuracy  model_3_eq_opp_diff  \\\n",
      "repeats                                                                \n",
      "Repeat_1            -0.034410          0.680000            -0.034410   \n",
      "Repeat_2             0.000000          0.663333             0.000000   \n",
      "Repeat_3            -0.020502          0.696667            -0.020502   \n",
      "Repeat_4             0.000000          0.703333             0.005848   \n",
      "Repeat_5            -0.008692          0.716667            -0.008692   \n",
      "\n",
      "          model_4_accuracy  model_4_eq_opp_diff  model_5_accuracy  \\\n",
      "repeats                                                             \n",
      "Repeat_1          0.680000            -0.034410          0.680000   \n",
      "Repeat_2          0.663333             0.000000          0.663333   \n",
      "Repeat_3          0.696667            -0.020502          0.696667   \n",
      "Repeat_4          0.703333             0.005848          0.703333   \n",
      "Repeat_5          0.713333            -0.008692          0.713333   \n",
      "\n",
      "          model_5_eq_opp_diff  model_6_accuracy  model_6_eq_opp_diff  \n",
      "repeats                                                               \n",
      "Repeat_1            -0.034410          0.680000            -0.034410  \n",
      "Repeat_2             0.000000          0.663333             0.000000  \n",
      "Repeat_3            -0.020502          0.696667            -0.020502  \n",
      "Repeat_4             0.005848          0.706667             0.000000  \n",
      "Repeat_5            -0.008692          0.716667            -0.008692  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_1_accuracy</th>\n",
       "      <th>model_1_eq_opp_diff</th>\n",
       "      <th>model_2_accuracy</th>\n",
       "      <th>model_2_eq_opp_diff</th>\n",
       "      <th>model_3_accuracy</th>\n",
       "      <th>model_3_eq_opp_diff</th>\n",
       "      <th>model_4_accuracy</th>\n",
       "      <th>model_4_eq_opp_diff</th>\n",
       "      <th>model_5_accuracy</th>\n",
       "      <th>model_5_eq_opp_diff</th>\n",
       "      <th>model_6_accuracy</th>\n",
       "      <th>model_6_eq_opp_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.692000</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>0.692667</td>\n",
       "      <td>-0.012721</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>-0.011551</td>\n",
       "      <td>0.691333</td>\n",
       "      <td>-0.011551</td>\n",
       "      <td>0.691333</td>\n",
       "      <td>-0.011551</td>\n",
       "      <td>0.692667</td>\n",
       "      <td>-0.012721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.019734</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.020763</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.019805</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.019805</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.014755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_1_accuracy  model_1_eq_opp_diff  model_2_accuracy  \\\n",
       "mean          0.692000            -0.019674          0.692667   \n",
       "std           0.020358             0.019734          0.021266   \n",
       "\n",
       "      model_2_eq_opp_diff  model_3_accuracy  model_3_eq_opp_diff  \\\n",
       "mean            -0.012721          0.692000            -0.011551   \n",
       "std              0.014755          0.020763             0.016179   \n",
       "\n",
       "      model_4_accuracy  model_4_eq_opp_diff  model_5_accuracy  \\\n",
       "mean          0.691333            -0.011551          0.691333   \n",
       "std           0.019805             0.016179          0.019805   \n",
       "\n",
       "      model_5_eq_opp_diff  model_6_accuracy  model_6_eq_opp_diff  \n",
       "mean            -0.011551          0.692667            -0.012721  \n",
       "std              0.016179          0.021266             0.014755  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds_results.to_csv('final0_nsf.csv')\n",
    "print(ds_nsf_results)\n",
    "mean = pd.Series(ds_nsf_results.mean(), name='mean')\n",
    "std = pd.Series(ds_nsf_results.std(), name='std')\n",
    "final_stats = pd.concat([mean.to_frame(), std.to_frame()], axis=1).transpose()\n",
    "final_stats.to_csv('final4_german_nsf.csv')\n",
    "final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cw2_DINGKE1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
